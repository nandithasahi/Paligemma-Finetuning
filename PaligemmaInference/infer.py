import torch
from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration
from peft import PeftModel
from PIL import Image
import os

# ğŸ”§ Configuration
BASE_MODEL = "models/paligemma-3b-mix-224"
LORA_DIR = os.path.join("output", "paligemma-finetuned")
IMAGE_PATH = "C:/Users/nandi/myint/PaligemmaInference/data/panda.jpg" 
MAX_NEW_TOKENS = 100

def main():
    # ğŸ”„ Load model and processor
    print("ğŸ”„ Loading model and processor...")
    
    try:
        processor = PaliGemmaProcessor.from_pretrained(BASE_MODEL)
        base_model = PaliGemmaForConditionalGeneration.from_pretrained(BASE_MODEL)
        
        # Check if LoRA directory exists
        if not os.path.exists(LORA_DIR):
            print(f"âŒ LoRA directory not found: {LORA_DIR}")
            print("Please ensure the model has been fine-tuned first.")
            return
        
        print(f"ğŸ“‚ Loading LoRA weights from {LORA_DIR}...")
        model = PeftModel.from_pretrained(base_model, LORA_DIR)
        
        # ğŸ”€ Merge LoRA weights (important for inference)
        print("ğŸ”€ Merging LoRA weights...")
        model = model.merge_and_unload()
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return

    # âœ… Move model to device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ“± Using device: {device}")
    model = model.to(device).eval()

    # ğŸ–¼ï¸ Load and prepare image
    print(f"ğŸ“‚ Loading image from {IMAGE_PATH}...")
    
    if not os.path.exists(IMAGE_PATH):
        print(f"âŒ Image file not found: {IMAGE_PATH}")
        return
    
    try:
        image = Image.open(IMAGE_PATH).convert("RGB")
        print(f"âœ… Image loaded successfully. Size: {image.size}")
    except Exception as e:
        print(f"âŒ Error loading image: {e}")
        return

   
    prompt = "caption en"  
    
    print(f"ğŸ”¤ Using prompt: '{prompt}'")

    try:
       
        inputs = processor(
            images=image,
            text=prompt,
            return_tensors="pt"
        ).to(device)
        
        print("âœ… Input processing successful")
        print(f"ğŸ“Š Input shapes:")
        print(f"  - input_ids: {inputs['input_ids'].shape}")
        print(f"  - attention_mask: {inputs['attention_mask'].shape}")
        print(f"  - pixel_values: {inputs['pixel_values'].shape}")
        
    except Exception as e:
        print(f"âŒ Error processing inputs: {e}")
        return

    # ğŸš€ Generate caption
    print("\nğŸš€ Running inference...")
    
    try:
        with torch.no_grad():
            generated_ids = model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                pixel_values=inputs["pixel_values"],
                max_new_tokens=MAX_NEW_TOKENS,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=processor.tokenizer.pad_token_id,
                eos_token_id=processor.tokenizer.eos_token_id,
                repetition_penalty=1.2,  # Reduced from 3.0
                no_repeat_ngram_size=3
            )
        
        print("âœ… Generation completed successfully")
        
    except Exception as e:
        print(f"âŒ Error during generation: {e}")
        return

    # ğŸ” Decode output -
    try:
        # Get only the generated tokens (exclude input tokens)
        input_length = inputs["input_ids"].shape[1]
        generated_tokens = generated_ids[:, input_length:]
        
        # Decode the generated tokens
        decoded_output = processor.batch_decode(
            generated_tokens, 
            skip_special_tokens=True
        )[0].strip()
        
        # Alternative: Decode full output and extract meaningful part
        full_decoded = processor.batch_decode(
            generated_ids, 
            skip_special_tokens=True
        )[0].strip()
        
        print("\n" + "="*50)
        print("ğŸ” RESULTS:")
        print("="*50)
        print(f"ğŸ“ Generated tokens only: '{decoded_output}'")
        print(f"ğŸ“„ Full decoded output: '{full_decoded}'")
        
        # Try to extract meaningful caption
        if decoded_output and decoded_output.strip():
            print(f"âœ… Predicted Caption: {decoded_output}")
        elif full_decoded and full_decoded.strip():
            # Try to extract caption after <image> token
            if "<image>" in full_decoded:
                caption = full_decoded.split("<image>")[-1].strip()
                if caption:
                    print(f"âœ… Extracted Caption: {caption}")
                else:
                    print("âš ï¸ No caption found after <image> token")
            else:
                print(f"âœ… Full Caption: {full_decoded}")
        else:
            print("âš ï¸ No meaningful caption generated")
            
    except Exception as e:
        print(f"âŒ Error decoding output: {e}")
        return


if __name__ == "__main__":
    main()